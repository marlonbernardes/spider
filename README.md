# ğŸ•·ï¸  Spider

Web crawler implementation for Monzo.

## First-time setup

```sh
# This will start kafka, elastic search and redis.
docker-compose up -d
yarn
yarn setup
yarn start
```

## Tasks

| Task         | Description                               |
|--------------|-------------------------------------------|
| start        | Starts the CLI                            |
| start:watch  | Starts the CLI and watches for changes    |
| db:reset     | Resets elastic search data/index to its initial state |
| lint         | Lints the code using eslint               |
| test         | Runs the unit/integration tests           |
| test --watch | Runs the tests in watch mode              |


# Architecture overview

This architecture diagram is still a WIP and is subject to change.

![architecture](./docs/architecture_v1.png)

### Components

  - **Redis**: used for caching which pages have been visited
  - **ElasticSearch**: used to persist & index the crawled documents
  - **Kafka**: used as a message queue for distributing work (a.k.a pages that need to be crawled) between multiple consumers.
  - **Crawling agent**: performs HTTP requests to the target URL(s), parses the response and adds child links to a Kafka queue, so linked pages
  can also be crawled. Caches visited pages in Redis (in order speed things up and to prevent cycles) and index the parsed information in ElasticSearch.
  - **Search**: as of this moment is implemented as part of the crawling agent and can be invoked via CLI (refer to the demo above).


## Directory structure

```sh
.
â”œâ”€â”€ config
â”œâ”€â”€ docs
â”œâ”€â”€ scripts # application scripts (e.g elastic search set-up)
â””â”€â”€ src
    â”œâ”€â”€ config # general configuration files
    â”œâ”€â”€ crawler # where the crawler logic resides
    â”œâ”€â”€ lib
    â”‚Â Â  â”œâ”€â”€ cache # interfaces for dealing with the "visited pages" cache
    â”‚Â Â  â”œâ”€â”€ http # http client/interfaces
    â”‚Â Â  â”œâ”€â”€ parser # parsers for each content type (currently only text/html) is supported
    â”‚Â Â  â””â”€â”€ queue # kafka consumer/producer logic
    â”œâ”€â”€ repository # persistence layer
    â””â”€â”€ tasks # tasks executed when running the code via the CLI
```

## Known limitations

- Robots.txt is not (yet) being respected
- sitemap.xml is not being used
- Performance could be improved by ignoring files with unsupported media types
