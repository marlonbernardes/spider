# Name of the topics containing the URLS which need to be crawled
KAFKA_TOPIC_NAME=spider.crawling_queue

# List of Kafka brokers - should be separated by a semicolon (e.g
# localhost:9092;localhost:9093)
KAFKA_BROKERS=localhost:9092

# Number of messages a single consumer can process concurrently
# See: https://kafka.js.org/docs/consuming#a-name-concurrent-processing-a-partition-aware-concurrency
KAFKA_CONSUMER_CONCURRENCY=3

# Redis is used to store which pages have been visited,
# in order to improve performance (as it prevents the same page for being crawled twice) and also to short-circuit what could become and endless cycle
# (e.g page a.html references b.html and b.html references a.html)
REDIS_HOST=localhost
REDIS_PORT=6379

# Elastic search is used to index information about the pages that have been crawled.
ELASTIC_SEARCH_ENDPOINT=http://localhost:9200

# Indicates wether or not external links should be included when crawling
CRAWLER_INCLUDE_EXTERNAL_LINKS=false

# CSS selector used to obtain the child links of each crawled page.
CRAWLER_LINKS_SELECTOR=[href]
